{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = sm_session.boto_region_name\n",
    "account = sm_session.account_id()\n",
    "\n",
    "\n",
    "container_name = \"deepspeed-sm\"\n",
    "container_tag = \"latest\"\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/{}:{}'.format(account, region, container_name, container_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin 763104351884.dkr.ecr.{region}.amazonaws.com\n",
    "# loging to your private ECR\n",
    "!aws ecr get-login-password --region {region} | docker login --username AWS --password-stdin {account}.dkr.ecr.{region}.amazonaws.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working in region us-east-1\n",
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "Sending build context to Docker daemon  604.2kB\n",
      "Step 1/35 : ARG REGION\n",
      "Step 2/35 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:1.6.0-gpu-py36-cu101-ubuntu16.04\n",
      " ---> 0d7223a296dd\n",
      "Step 3/35 : ENV STAGE_DIR=/tmp\n",
      " ---> Using cache\n",
      " ---> 162c609ace50\n",
      "Step 4/35 : RUN mkdir -p ${STAGE_DIR}\n",
      " ---> Using cache\n",
      " ---> f0877b798876\n",
      "Step 5/35 : RUN apt-get update &&     apt-get install -y --no-install-recommends         software-properties-common autotools-dev         nfs-common pdsh g++ gcc tmux less unzip         htop iftop iotop rsync iputils-ping         net-tools         sudo\n",
      " ---> Using cache\n",
      " ---> eb3c8042f059\n",
      "Step 6/35 : RUN wget https://apt.llvm.org/llvm.sh &&     chmod +x llvm.sh &&     sudo ./llvm.sh 9\n",
      " ---> Using cache\n",
      " ---> 88f58e7a21a9\n",
      "Step 7/35 : ENV DEBIAN_FRONTEND=noninteractive\n",
      " ---> Using cache\n",
      " ---> c8e099f4aa65\n",
      "Step 8/35 : ENV PYTHON_VERSION=3\n",
      " ---> Using cache\n",
      " ---> 9a87dde03905\n",
      "Step 9/35 : RUN pip install pyyaml\n",
      " ---> Using cache\n",
      " ---> e5b7f443d399\n",
      "Step 10/35 : RUN pip install ipython\n",
      " ---> Using cache\n",
      " ---> 00975de1c97e\n",
      "Step 11/35 : ENV TENSORFLOW_VERSION=1.15.2\n",
      " ---> Using cache\n",
      " ---> bada2b0b7d81\n",
      "Step 12/35 : RUN pip install tensorflow-gpu==${TENSORFLOW_VERSION}\n",
      " ---> Using cache\n",
      " ---> 9846f4f280ac\n",
      "Step 13/35 : RUN apt-get update &&     apt-get install -y --no-install-recommends         libsndfile-dev         libcupti-dev         libjpeg-dev         libpng-dev         screen\n",
      " ---> Using cache\n",
      " ---> 5489daccb259\n",
      "Step 14/35 : RUN pip install yappi                 cffi                 ipdb                 py3nvml                 pyarrow                 graphviz                 astor                 tqdm                 sentencepiece                 msgpack                 sphinx                 sphinx_rtd_theme                 nvidia-ml-py3                 mpi4py                 cupy-cuda101\n",
      " ---> Using cache\n",
      " ---> d0f2df1a49a5\n",
      "Step 15/35 : RUN rm -rf /usr/lib/python3/dist-packages/yaml &&     rm -rf /usr/lib/python3/dist-packages/PyYAML-*\n",
      " ---> Using cache\n",
      " ---> 07414301da21\n",
      "Step 16/35 : RUN useradd --create-home --uid 1000 --shell /bin/bash deepspeed\n",
      " ---> Using cache\n",
      " ---> cd76bb4cb55c\n",
      "Step 17/35 : RUN usermod -aG sudo deepspeed\n",
      " ---> Using cache\n",
      " ---> 5ec7c29197b8\n",
      "Step 18/35 : RUN echo \"deepspeed ALL=(ALL) NOPASSWD: ALL\" >> /etc/sudoers\n",
      " ---> Using cache\n",
      " ---> 9637d92a3054\n",
      "Step 19/35 : USER deepspeed\n",
      " ---> Using cache\n",
      " ---> fd078e6f78e7\n",
      "Step 20/35 : ENV CUDA_HOME=/usr/local/cuda-10.1\n",
      " ---> Using cache\n",
      " ---> 28e132b4adcc\n",
      "Step 21/35 : ADD https://raw.githubusercontent.com/aws/deep-learning-containers/master/src/deep_learning_container.py /usr/local/bin/deep_learning_container.py\n",
      "Downloading  4.739kB\n",
      " ---> Using cache\n",
      " ---> 15da416959f4\n",
      "Step 22/35 : RUN sudo chmod ugo+rwx /usr/local/bin/deep_learning_container.py\n",
      " ---> Using cache\n",
      " ---> abaa01bfeca4\n",
      "Step 23/35 : RUN git clone https://github.com/microsoft/DeepSpeed.git ${STAGE_DIR}/DeepSpeed\n",
      " ---> Using cache\n",
      " ---> ab8db2d80f27\n",
      "Step 24/35 : USER root\n",
      " ---> Using cache\n",
      " ---> 9fbf26d6cffa\n",
      "Step 25/35 : ENV PATH=/home/deepspeed/.local/bin:${PATH}\n",
      " ---> Using cache\n",
      " ---> d6c2c9e34d17\n",
      "Step 26/35 : RUN sudo echo 'Defaults secure_path=    /opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin'    >> /etc/sudoers.d/deepspeed_config\n",
      " ---> Using cache\n",
      " ---> ffb281ecfa5d\n",
      "Step 27/35 : USER deepspeed\n",
      " ---> Using cache\n",
      " ---> b61ae9ac0621\n",
      "Step 28/35 : RUN cd ${STAGE_DIR}/DeepSpeed &&     git checkout . &&     git checkout master &&     ./install.sh -s\n",
      " ---> Using cache\n",
      " ---> 32ab94bdfeef\n",
      "Step 29/35 : RUN rm -rf ${STAGE_DIR}/DeepSpeed\n",
      " ---> Using cache\n",
      " ---> 951283b66f95\n",
      "Step 30/35 : RUN python -c \"import deepspeed; print(deepspeed.__version__)\"\n",
      " ---> Using cache\n",
      " ---> 5e63106b675d\n",
      "Step 31/35 : COPY training_container /opt/ml/code\n",
      " ---> aeb396b39b99\n",
      "Step 32/35 : ENV SAGEMAKER_SUBMIT_DIRECTORY /opt/ml/code\n",
      " ---> Running in 308c2cb5c10d\n",
      "Removing intermediate container 308c2cb5c10d\n",
      " ---> 5ea151c7234e\n",
      "Step 33/35 : ENV SAGEMAKER_PROGRAM ds_launcher.py\n",
      " ---> Running in a99fc0462491\n",
      "Removing intermediate container a99fc0462491\n",
      " ---> b774ed950428\n",
      "Step 34/35 : USER root\n",
      " ---> Running in c5f5b92a8c42\n",
      "Removing intermediate container c5f5b92a8c42\n",
      " ---> cc1b3f59d9b2\n",
      "Step 35/35 : WORKDIR /\n",
      " ---> Running in 40311c4f93b8\n",
      "Removing intermediate container 40311c4f93b8\n",
      " ---> 73f004fb69fe\n",
      "Successfully built 73f004fb69fe\n",
      "Successfully tagged deepspeed-sm:latest\n",
      "The push refers to repository [553020858742.dkr.ecr.us-east-1.amazonaws.com/deepspeed-sm]\n",
      "\n",
      "\u001b[1B31569338: Preparing \n",
      "\u001b[1B2bcec8fa: Preparing \n",
      "\u001b[1Bd201aff5: Preparing \n",
      "\u001b[1Bd44b8ba2: Preparing \n",
      "\u001b[1Bee78edbe: Preparing \n",
      "\u001b[1B9daf192b: Preparing \n",
      "\u001b[1B8591b2c6: Preparing \n",
      "\u001b[1B6866874b: Preparing \n",
      "\u001b[1B939c0908: Preparing \n",
      "\u001b[1B7b66cd00: Preparing \n",
      "\u001b[1B61fa2ece: Preparing \n",
      "\u001b[1Ba657be39: Preparing \n",
      "\u001b[1Ba440cf5b: Preparing \n",
      "\u001b[1B864f0dc1: Preparing \n",
      "\u001b[1Baa859dc0: Preparing \n",
      "\u001b[1Bb8d69f3a: Preparing \n",
      "\u001b[1B319f6d23: Preparing \n",
      "\u001b[1Bc7aa76c8: Preparing \n",
      "\u001b[1B1fe1bdd8: Preparing \n",
      "\u001b[1Bf8f27e93: Preparing \n",
      "\u001b[1Be2c51fb8: Preparing \n",
      "\u001b[1Bb53f6a90: Preparing \n",
      "\u001b[1B3d9e777f: Preparing \n",
      "\u001b[1B60d3d57f: Preparing \n",
      "\u001b[1B12dede71: Preparing \n",
      "\u001b[1Bc86c5717: Preparing \n",
      "\u001b[22Bdaf192b: Waiting g \n",
      "\u001b[1Be636b2d2: Preparing \n",
      "\u001b[20Bb66cd00: Waiting g \n",
      "\u001b[1B974d8866: Preparing \n",
      "\u001b[1B4adf4e7a: Preparing \n",
      "\u001b[22B1fa2ece: Waiting g \n",
      "\u001b[1Bc0f7ff68: Preparing \n",
      "\u001b[26B39c0908: Waiting g \n",
      "\u001b[28B866874b: Waiting g \n",
      "\u001b[14Bd9e777f: Waiting g \n",
      "\u001b[21B19f6d23: Waiting g \n",
      "\u001b[21B7aa76c8: Waiting g \n",
      "\u001b[11B982472c: Waiting g \n",
      "\u001b[15B86c5717: Waiting g \n",
      "\u001b[23Bfe1bdd8: Waiting g \n",
      "\u001b[15B636b2d2: Waiting g \n",
      "\u001b[1B29ee6e59: Preparing \n",
      "\u001b[24B2c51fb8: Waiting g \n",
      "\u001b[6Ba5f70275: Waiting g \n",
      "\u001b[1Bd0e0bf70: Layer already exists 5kB1A\u001b[2K\u001b[37A\u001b[2K\u001b[33A\u001b[2K\u001b[28A\u001b[2K\u001b[23A\u001b[2K\u001b[19A\u001b[2K\u001b[14A\u001b[2K\u001b[8A\u001b[2K\u001b[5A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:10a4ca5546335814337bda50092cc71e88a395cb6e1ade128da3b376ec233fb1 size: 9971\n"
     ]
    }
   ],
   "source": [
    "! ./build_and_push.sh $container_name $container_tag Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker images ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install 'sagemaker[local]' --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.local import LocalSession\n",
    "\n",
    "# Configure our local training session\n",
    "sagemaker_local_session = LocalSession()\n",
    "sagemaker_local_session.config = {'local': {'local_code': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_config={\n",
    "  \"train_batch_size\": 8,\n",
    "  \"gradient_accumulation_steps\": 1,\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"Adam\",\n",
    "    \"params\": {\n",
    "      \"lr\": 0.00015\n",
    "    }\n",
    "  },\n",
    "  \"fp16\": {\n",
    "    \"enabled\": True\n",
    "  },\n",
    "  \"zero_optimization\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = sagemaker.estimator.Estimator(image,\n",
    "                                    role=role,\n",
    "                                    instance_count=1,\n",
    "                                    instance_type='local_gpu',\n",
    "                                    hyperparameters = ds_config,\n",
    "                                    sagemaker_session=sagemaker_local_session\n",
    ")\n",
    "\n",
    "est.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remote start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-11 23:37:14 Starting - Starting the training job...\n",
      "2021-02-11 23:37:39 Starting - Launching requested ML instancesProfilerReport-1613086634: InProgress\n",
      ".........\n",
      "2021-02-11 23:39:02 Starting - Preparing the instances for training.........\n",
      "2021-02-11 23:40:41 Downloading - Downloading input data\n",
      "2021-02-11 23:40:41 Training - Downloading the training image....................\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-02-11 23:43:57,285 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-02-11 23:43:57,352 sagemaker-training-toolkit INFO     Failed to parse hyperparameter optimizer value {'type': 'Adam', 'params': {'lr': 0.00015}} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:43:57,352 sagemaker-training-toolkit INFO     Failed to parse hyperparameter zero_optimization value True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:43:57,352 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fp16 value {'enabled': True} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:43:57,364 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\n",
      "2021-02-11 23:44:09 Training - Training image download completed. Training in progress.\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:04,752 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:04,818 sagemaker-training-toolkit INFO     Failed to parse hyperparameter optimizer value {'type': 'Adam', 'params': {'lr': 0.00015}} to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:04,818 sagemaker-training-toolkit INFO     Failed to parse hyperparameter zero_optimization value True to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:04,818 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fp16 value {'enabled': True} to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:04,830 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,077 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,145 sagemaker-training-toolkit INFO     Failed to parse hyperparameter optimizer value {'type': 'Adam', 'params': {'lr': 0.00015}} to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,145 sagemaker-training-toolkit INFO     Failed to parse hyperparameter zero_optimization value True to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,145 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fp16 value {'enabled': True} to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,222 sagemaker-training-toolkit INFO     Failed to parse hyperparameter optimizer value {'type': 'Adam', 'params': {'lr': 0.00015}} to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,222 sagemaker-training-toolkit INFO     Failed to parse hyperparameter zero_optimization value True to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,222 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fp16 value {'enabled': True} to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,301 sagemaker-training-toolkit INFO     Failed to parse hyperparameter optimizer value {'type': 'Adam', 'params': {'lr': 0.00015}} to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,301 sagemaker-training-toolkit INFO     Failed to parse hyperparameter zero_optimization value True to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,301 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fp16 value {'enabled': True} to Json.\u001b[0m\n",
      "\u001b[35mReturning the value itself\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,314 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 8,\n",
      "        \"optimizer\": \"{'type': 'Adam', 'params': {'lr': 0.00015}}\",\n",
      "        \"zero_optimization\": \"True\",\n",
      "        \"fp16\": \"{'enabled': True}\",\n",
      "        \"gradient_accumulation_steps\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"deepspeed-sm-2021-02-11-23-37-14-008\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"ds_launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds_launcher.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"fp16\":\"{'enabled': True}\",\"gradient_accumulation_steps\":1,\"optimizer\":\"{'type': 'Adam', 'params': {'lr': 0.00015}}\",\"train_batch_size\":8,\"zero_optimization\":\"True\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=ds_launcher.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=ds_launcher\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"fp16\":\"{'enabled': True}\",\"gradient_accumulation_steps\":1,\"optimizer\":\"{'type': 'Adam', 'params': {'lr': 0.00015}}\",\"train_batch_size\":8,\"zero_optimization\":\"True\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"deepspeed-sm-2021-02-11-23-37-14-008\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"ds_launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds_launcher.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--fp16\",\"{'enabled': True}\",\"--gradient_accumulation_steps\",\"1\",\"--optimizer\",\"{'type': 'Adam', 'params': {'lr': 0.00015}}\",\"--train_batch_size\",\"8\",\"--zero_optimization\",\"True\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[35mSM_HP_OPTIMIZER={'type': 'Adam', 'params': {'lr': 0.00015}}\u001b[0m\n",
      "\u001b[35mSM_HP_ZERO_OPTIMIZATION=True\u001b[0m\n",
      "\u001b[35mSM_HP_FP16={'enabled': True}\u001b[0m\n",
      "\u001b[35mSM_HP_GRADIENT_ACCUMULATION_STEPS=1\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python ds_launcher.py --fp16 {'enabled': True} --gradient_accumulation_steps 1 --optimizer {'type': 'Adam', 'params': {'lr': 0.00015}} --train_batch_size 8 --zero_optimization True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35minside main func\u001b[0m\n",
      "\u001b[35mIs master node=False\u001b[0m\n",
      "\u001b[35mNon master node. Stopping the execution.\u001b[0m\n",
      "\u001b[35m2021-02-11 23:44:05,364 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:09,999 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:10,066 sagemaker-training-toolkit INFO     Failed to parse hyperparameter optimizer value {'type': 'Adam', 'params': {'lr': 0.00015}} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:10,067 sagemaker-training-toolkit INFO     Failed to parse hyperparameter zero_optimization value True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:10,067 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fp16 value {'enabled': True} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:10,145 sagemaker-training-toolkit INFO     Failed to parse hyperparameter optimizer value {'type': 'Adam', 'params': {'lr': 0.00015}} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:10,145 sagemaker-training-toolkit INFO     Failed to parse hyperparameter zero_optimization value True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:10,145 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fp16 value {'enabled': True} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:10,222 sagemaker-training-toolkit INFO     Failed to parse hyperparameter optimizer value {'type': 'Adam', 'params': {'lr': 0.00015}} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:10,222 sagemaker-training-toolkit INFO     Failed to parse hyperparameter zero_optimization value True to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:10,222 sagemaker-training-toolkit INFO     Failed to parse hyperparameter fp16 value {'enabled': True} to Json.\u001b[0m\n",
      "\u001b[34mReturning the value itself\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:10,234 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"train_batch_size\": 8,\n",
      "        \"optimizer\": \"{'type': 'Adam', 'params': {'lr': 0.00015}}\",\n",
      "        \"zero_optimization\": \"True\",\n",
      "        \"fp16\": \"{'enabled': True}\",\n",
      "        \"gradient_accumulation_steps\": 1\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"deepspeed-sm-2021-02-11-23-37-14-008\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/code\",\n",
      "    \"module_name\": \"ds_launcher\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 64,\n",
      "    \"num_gpus\": 8,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"ds_launcher.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"fp16\":\"{'enabled': True}\",\"gradient_accumulation_steps\":1,\"optimizer\":\"{'type': 'Adam', 'params': {'lr': 0.00015}}\",\"train_batch_size\":8,\"zero_optimization\":\"True\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=ds_launcher.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=ds_launcher\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=64\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/code\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"fp16\":\"{'enabled': True}\",\"gradient_accumulation_steps\":1,\"optimizer\":\"{'type': 'Adam', 'params': {'lr': 0.00015}}\",\"train_batch_size\":8,\"zero_optimization\":\"True\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"deepspeed-sm-2021-02-11-23-37-14-008\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"ds_launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds_launcher.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--fp16\",\"{'enabled': True}\",\"--gradient_accumulation_steps\",\"1\",\"--optimizer\",\"{'type': 'Adam', 'params': {'lr': 0.00015}}\",\"--train_batch_size\",\"8\",\"--zero_optimization\",\"True\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER={'type': 'Adam', 'params': {'lr': 0.00015}}\u001b[0m\n",
      "\u001b[34mSM_HP_ZERO_OPTIMIZATION=True\u001b[0m\n",
      "\u001b[34mSM_HP_FP16={'enabled': True}\u001b[0m\n",
      "\u001b[34mSM_HP_GRADIENT_ACCUMULATION_STEPS=1\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python ds_launcher.py --fp16 {'enabled': True} --gradient_accumulation_steps 1 --optimizer {'type': 'Adam', 'params': {'lr': 0.00015}} --train_batch_size 8 --zero_optimization True\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34minside main func\u001b[0m\n",
      "\u001b[34mIs master node=True\u001b[0m\n",
      "\u001b[34mHostfile /opt/ml/input/config/hostfile has been saved.\u001b[0m\n",
      "\u001b[34mConfig file /opt/ml/input/config/ds_config.json saved.\u001b[0m\n",
      "\u001b[34mFollowing command line will be launched on master:['deepspeed', '--hostfile /opt/ml/input/config/hostfile', '/opt/ml/code/ds_launcher.py', '--launcher openmpi', '--deepspeed', '--deepspeed_config /opt/ml/input/config/ds_config.json']\u001b[0m\n",
      "\u001b[34mEnv variables to be distributed across nodes: {'LD_PRELOAD': '/libchangehostname.so', 'SAGEMAKER_PROGRAM': 'ds_launcher.py', 'CUDNN_VERSION': '7.6.5.32', 'HOSTNAME': 'ip-10-2-94-160.ec2.internal', 'TRAINING_JOB_NAME': 'deepspeed-sm-2021-02-11-23-37-14-008', 'NVIDIA_REQUIRE_CUDA': 'cuda>=10.1 brand=tesla,driver>=396,driver<397 brand=tesla,driver>=410,driver<411 brand=tesla,driver>=418,driver<419', 'TRAINING_JOB_ARN': 'arn:aws:sagemaker:us-east-1:553020858742:training-job/deepspeed-sm-2021-02-11-23-37-14-008', 'SAGEMAKER_TRAINING_MODULE': 'sagemaker_pytorch_container.training:main', 'AWS_CONTAINER_CREDENTIALS_RELATIVE_URI': '/v2/credentials/0f3f6a3a-23be-4251-a49c-e26cae7c3f98', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'PYTHONUNBUFFERED': '1', 'LC_ALL': 'C.UTF-8', 'CUDA_HOME': '/usr/local/cuda-10.1', 'PYTHONIOENCODING': 'UTF-8', 'LD_LIBRARY_PATH': '/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/lib:/opt/conda/lib:/home/.openmpi/lib/', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility,compat32,graphics,video', 'HOROVOD_VERSION': '0.19.5', 'AWS_EXECUTION_ENV': 'AWS_ECS_EC2', 'PYTHON_VERSION': '3', 'PATH': '/home/deepspeed/.local/bin:/opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/home/.openmpi/bin', 'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/code', 'PWD': '/', 'TORCH_NVCC_FLAGS': '-Xfatbin -compress-all', 'LANG': 'C.UTF-8', 'STAGE_DIR': '/tmp', 'TORCH_CUDA_ARCH_LIST': '3.5 5.2 6.0 6.1 7.0+PTX', 'AWS_REGION': 'us-east-1', 'CUDA_PKG_VERSION': '10-1=10.1.243-1', 'CUDA_VERSION': '10.1.243', 'PYTHONDONTWRITEBYTECODE': '1', 'SHLVL': '1', 'HOME': '/root', 'NCCL_VERSION': '2.7.8', 'DEBIAN_FRONTEND': 'noninteractive', 'DGLBACKEND': 'pytorch', 'TENSORFLOW_VERSION': '1.15.2', 'CMAKE_PREFIX_PATH': '$(dirname $(which conda))/../', 'DMLC_INTERFACE': 'eth0', 'ECS_CONTAINER_METADATA_URI': 'http://169.254.170.2/v3/8cd44120-6363-4d07-9fd3-a6474bb03539', 'ECS_CONTAINER_METADATA_URI_V4': 'http://169.254.170.2/v4/8cd44120-6363-4d07-9fd3-a6474bb03539', '_': '/opt/conda/bin/train', 'SAGEMAKER_JOB_NAME': '', 'CURRENT_HOST': 'algo-1', 'SAGEMAKER_REGION': '', 'NCCL_SOCKET_IFNAME': 'eth0', 'NCCL_IB_DISABLE': '1', 'NCCL_DEBUG': 'WARN', 'MASTER_ADDR': 'algo-1', 'MASTER_PORT': '7777', 'SM_HOSTS': '[\"algo-1\",\"algo-2\"]', 'SM_NETWORK_INTERFACE_NAME': 'eth0', 'SM_HPS': '{\"fp16\":\"{\\'enabled\\': True}\",\"gradient_accumulation_steps\":1,\"optimizer\":\"{\\'type\\': \\'Adam\\', \\'params\\': {\\'lr\\': 0.00015}}\",\"train_batch_size\":8,\"zero_optimization\":\"True\"}', 'SM_USER_ENTRY_POINT': 'ds_launcher.py', 'SM_FRAMEWORK_PARAMS': '{}', 'SM_RESOURCE_CONFIG': '{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}', 'SM_INPUT_DATA_CONFIG': '{}', 'SM_OUTPUT_DATA_DIR': '/opt/ml/output/data', 'SM_CHANNELS': '[]', 'SM_CURRENT_HOST': 'algo-1', 'SM_MODULE_NAME': 'ds_launcher', 'SM_LOG_LEVEL': '20', 'SM_FRAMEWORK_MODULE': 'sagemaker_pytorch_container.training:main', 'SM_INPUT_DIR': '/opt/ml/input', 'SM_INPUT_CONFIG_DIR': '/opt/ml/input/config', 'SM_OUTPUT_DIR': '/opt/ml/output', 'SM_NUM_CPUS': '64', 'SM_NUM_GPUS': '8', 'SM_MODEL_DIR': '/opt/ml/model', 'SM_MODULE_DIR': '/opt/ml/code', 'SM_TRAINING_ENV': '{\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"fp16\":\"{\\'enabled\\': True}\",\"gradient_accumulation_steps\":1,\"optimizer\":\"{\\'type\\': \\'Adam\\', \\'params\\': {\\'lr\\': 0.00015}}\",\"train_batch_size\":8,\"zero_optimization\":\"True\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"deepspeed-sm-2021-02-11-23-37-14-008\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/code\",\"module_name\":\"ds_launcher\",\"network_interface_name\":\"eth0\",\"num_cpus\":64,\"num_gpus\":8,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"ds_launcher.py\"}', 'SM_USER_ARGS': '[\"--fp16\",\"{\\'enabled\\': True}\",\"--gradient_accumulation_steps\",\"1\",\"--optimizer\",\"{\\'type\\': \\'Adam\\', \\'params\\': {\\'lr\\': 0.00015}}\",\"--train_batch_size\",\"8\",\"--zero_optimization\",\"True\"]', 'SM_OUTPUT_INTERMEDIATE_DIR': '/opt/ml/output/intermediate', 'SM_HP_TRAIN_BATCH_SIZE': '8', 'SM_HP_OPTIMIZER': \"{'type': 'Adam', 'params': {'lr': 0.00015}}\", 'SM_HP_ZERO_OPTIMIZATION': 'True', 'SM_HP_FP16': \"{'enabled': True}\", 'SM_HP_GRADIENT_ACCUMULATION_STEPS': '1', 'PYTHONPATH': '/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages'}\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:13,687] [WARNING] [runner.py:117:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:14,308] [INFO] [runner.py:358:main] cmd = /opt/conda/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --hostfile /opt/ml/input/config/hostfile /opt/ml/code/ds_launcher.py --launcher openmpi --deepspeed --deepspeed_config /opt/ml/input/config/ds_config.json\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:15,647] [INFO] [launch.py:73:main] 0 NCCL_VERSION 2.7.8\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:15,647] [INFO] [launch.py:73:main] 0 NCCL_SOCKET_IFNAME eth0\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:15,647] [INFO] [launch.py:73:main] 0 NCCL_IB_DISABLE 1\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:15,647] [INFO] [launch.py:73:main] 0 NCCL_DEBUG WARN\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:15,647] [INFO] [launch.py:80:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:15,647] [INFO] [launch.py:89:main] nnodes=1, num_local_procs=8, node_rank=0\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:15,647] [INFO] [launch.py:101:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:15,647] [INFO] [launch.py:102:main] dist_world_size=8\u001b[0m\n",
      "\u001b[34m[2021-02-11 23:44:15,647] [INFO] [launch.py:105:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\u001b[0m\n",
      "\u001b[34mUnknown option: --\u001b[0m\n",
      "\u001b[34musage: /opt/conda/bin/python [option] ... [-c cmd | -m mod | file | -] [arg] ...\u001b[0m\n",
      "\u001b[34mTry `python -h' for more information.\u001b[0m\n",
      "\u001b[34mUnknown option: --\u001b[0m\n",
      "\u001b[34musage: /opt/conda/bin/python [option] ... [-c cmd | -m mod | file | -] [arg] ...\u001b[0m\n",
      "\u001b[34mTry `python -h' for more information.\u001b[0m\n",
      "\u001b[34mUnknown option: --\u001b[0m\n",
      "\u001b[34musage: /opt/conda/bin/python [option] ... [-c cmd | -m mod | file | -] [arg] ...\u001b[0m\n",
      "\u001b[34mTry `python -h' for more information.\u001b[0m\n",
      "\u001b[34mUnknown option: --\u001b[0m\n",
      "\u001b[34musage: /opt/conda/bin/python [option] ... [-c cmd | -m mod | file | -] [arg] ...\u001b[0m\n",
      "\u001b[34mTry `python -h' for more information.\u001b[0m\n",
      "\u001b[34mUnknown option: --\u001b[0m\n",
      "\u001b[34musage: /opt/conda/bin/python [option] ... [-c cmd | -m mod | file | -] [arg] ...\u001b[0m\n",
      "\u001b[34mTry `python -h' for more information.\u001b[0m\n",
      "\u001b[34mUnknown option: --\u001b[0m\n",
      "\u001b[34musage: /opt/conda/bin/python [option] ... [-c cmd | -m mod | file | -] [arg] ...\u001b[0m\n",
      "\u001b[34mTry `python -h' for more information.\u001b[0m\n",
      "\u001b[34mUnknown option: --\u001b[0m\n",
      "\u001b[34musage: /opt/conda/bin/python [option] ... [-c cmd | -m mod | file | -] [arg] ...\u001b[0m\n",
      "\u001b[34mTry `python -h' for more information.\u001b[0m\n",
      "\u001b[34mKilling subprocess 163\u001b[0m\n",
      "\u001b[34mKilling subprocess 164\u001b[0m\n",
      "\u001b[34mKilling subprocess 165\u001b[0m\n",
      "\u001b[34mKilling subprocess 166\u001b[0m\n",
      "\u001b[34mKilling subprocess 167\u001b[0m\n",
      "\u001b[34mKilling subprocess 168\u001b[0m\n",
      "\u001b[34mKilling subprocess 169\u001b[0m\n",
      "\u001b[34mKilling subprocess 170\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\u001b[0m\n",
      "\u001b[34m\"__main__\", mod_spec)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\u001b[0m\n",
      "\u001b[34mexec(code, run_globals)\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/deepspeed/launcher/launch.py\", line 171, in <module>\u001b[0m\n",
      "\u001b[34mmain()\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/deepspeed/launcher/launch.py\", line 161, in main\u001b[0m\n",
      "\u001b[34msigkill_handler(signal.SIGTERM, None)  # not coming back\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.6/site-packages/deepspeed/launcher/launch.py\", line 139, in sigkill_handler\u001b[0m\n",
      "\u001b[34mraise subprocess.CalledProcessError(returncode=last_return_code, cmd=cmd)\u001b[0m\n",
      "\u001b[34msubprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', '--hostfile /opt/ml/input/config/hostfile', '--local_rank=7', '/opt/ml/code/ds_launcher.py', '--launcher openmpi', '--deepspeed', '--deepspeed_config /opt/ml/input/config/ds_config.json']' returned non-zero exit status 2.\u001b[0m\n",
      "\u001b[34m2021-02-11 23:44:16,407 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2021-02-11 23:44:29 Uploading - Uploading generated training model\n",
      "2021-02-11 23:44:29 Completed - Training job completed\n",
      "Training seconds: 476\n",
      "Billable seconds: 476\n"
     ]
    }
   ],
   "source": [
    "est = sagemaker.estimator.Estimator(image,\n",
    "                                    role=role,\n",
    "                                    instance_count=2,\n",
    "                                    instance_type='ml.p3.16xlarge',\n",
    "                                    hyperparameters = ds_config,\n",
    "                                    sagemaker_session=sm_session\n",
    ")\n",
    "\n",
    "est.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
